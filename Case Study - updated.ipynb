{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Prediciting airline passenger using recurrent neural networks in TF\n",
    "Download Data from:\n",
    "https://datamarket.com/data/set/22u3/international-airline-passengers-monthly-totals-in-thousands-jan-49-dec-60#!ds=22u3&display=line    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data:\n",
    "file = \"international-airline-passengers.csv\"\n",
    "airlines = pd.read_csv(file,  header = 0, names = ['Date', 'Passengers'])\n",
    "# Drop last line: \"International airline passengers: monthly totals in thousands. Jan 49 ? Dec 60\"\n",
    "airlines.drop(144, inplace = True)\n",
    "#Inspect Data\n",
    "print(airlines.head())\n",
    "dataset=np.expand_dims(airlines['Passengers'].values,1)\n",
    "# normalize the dataset to [0,1] range\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) #x_norm=(x-min)/(min-max)\n",
    "dataset_norm = scaler.fit_transform(dataset)[:,0]\n",
    "# Keep max and min to denormalize data later #x=x_norm*(min-max)+min\n",
    "max_dataset=np.max(dataset)\n",
    "min_dataset=np.min(dataset)\n",
    "# prepare data to feed to model:\n",
    "# each observation will be of length 'seq_size'\n",
    "# For example, seq_size=2 then\n",
    "# x[0]=[t_0,t_1]\n",
    "# x[1]=[t_1,t_2]\n",
    "# ...\n",
    "# As for Y, it will also be of length 'seq_size'. why? (hint, sequence-to-sequence)\n",
    "# For example, seq_size=2 then\n",
    "# y[0]=[t_1,t_2]\n",
    "# y[1]=[t_2,t_3]\n",
    "# ...\n",
    "seq_size = 12 # one year\n",
    "trainX, trainY = [], []\n",
    "for i in range(len(dataset_norm) - seq_size):\n",
    "    trainX.append(np.expand_dims(dataset_norm[i:i+seq_size], axis=1).tolist())\n",
    "    trainY.append(dataset_norm[i+1:i+seq_size+1])\n",
    "\n",
    "# Inspect trainX and trainY\n",
    "print(\"X:\\n\",'\\n'.join([str(item) for item in trainX[0:2]]))\n",
    "print(\"Y:\\n\",'\\n'.join([str(item) for item in trainY[0:2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build computational graph\n",
    "input_dim=1 # dim > 1 for multivariate time series\n",
    "hidden_dim=100 # number of hiddent units h\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # input place holders\n",
    "    # input Shape: [# training examples, sequence length, # features]\n",
    "    x = tf.placeholder(tf.float32,[None,seq_size,input_dim])\n",
    "    # label Shape: [# training examples, sequence length]\n",
    "    y = tf.placeholder(tf.float32,[None,seq_size])\n",
    "    \n",
    "    # RNN Network\n",
    "    cell = rnn.BasicRNNCell(hidden_dim)\n",
    "    \n",
    "    # RNN output Shape: [# training examples, sequence length, # hidden] \n",
    "    outputs, _ = tf.nn.dynamic_rnn(cell,x,dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # weights for output dense layer (i.e., after RNN)\n",
    "    # W shape: [# hidden, 1]\n",
    "    W_out = tf.Variable(tf.random_normal([hidden_dim,1]),name=\"w_out\") \n",
    "    # b shape: [1]\n",
    "    b_out = tf.Variable(tf.random_normal([1]),name=\"b_out\")\n",
    "\n",
    "    # output dense layer:\n",
    "    num_examples = tf.shape(x)[0] \n",
    "    # convert W from [# hidden, 1] to [# training examples, # hidden, 1]\n",
    "    # step 1: add a new dimension at index 0 using tf.expand_dims\n",
    "    w_exp= tf.expand_dims(W_out,0)\n",
    "    # step 2: duplicate W for 'num_examples' times using tf.tile\n",
    "    W_repeated = tf.tile(w_exp,[num_examples,1,1])\n",
    "    \n",
    "    # Dense Layer calculation: \n",
    "    # [# training examples, sequence length, # hidden] *\n",
    "    # [# training examples, # hidden, 1] = [# training examples, sequence length]\n",
    "    \n",
    "    y_pred = tf.matmul(outputs,W_repeated)+b_out\n",
    "    # Actually, y_pred: [# training examples, sequence length, 1]\n",
    "    # Remove last dimension using tf.squeeze\n",
    "    y_pred = tf.squeeze(y_pred)\n",
    "    \n",
    "    # Cost & Training Step\n",
    "    cost = tf.reduce_mean(tf.square(y_pred-y))\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run Session\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Run for 1000 iterations (1000 is arbitrary, need a validation set to tune!)\n",
    "    print('Training...')\n",
    "    for i in range(5000): # If we train more, would we overfit? Try 10000\n",
    "        _, train_err = sess.run([train_op,cost],feed_dict={x:trainX,y:trainY})\n",
    "        if i==0:\n",
    "            print('  step, train err= %6d: %8.5f' % (0,train_err)) \n",
    "        elif  (i+1) % 100 == 0: \n",
    "            print('  step, train err= %6d: %8.5f' % (i+1,train_err)) \n",
    "    # debugging section ---- won't be needed in actual model code\n",
    "   # np.set_printoptions(precision=3) # adjust precision for better printing\n",
    "   # print(\"First training example x:\",np.around(trainX[0], decimals=3))\n",
    "   # print(\"y:\",np.around(trainY[0], decimals=3))\n",
    "   # # run the session for all tensors that we want to debug: \n",
    "   # #x,y,outputs,num_examples,W_out,b_out,w_exp,W_repeated,y_pred,cost\n",
    "   # [x_v,y_v,outputs_v,num_examples_v,W_out_v,\n",
    "   #  b_out_v,w_exp_v,W_repeated_v,y_pred_v,cost_v]=sess.run([x,y,outputs,num_examples,\n",
    "   #                                                          W_out,b_out,w_exp,W_repeated,\n",
    "   #                                                          y_pred,cost],feed_dict={x:[trainX[0]],y:[trainY[0]]})\n",
    "   # print(\"x_v:\",x_v.shape,np.around(x_v, decimals=3))\n",
    "   # print(\"y_v:\",y_v.shape,np.around(y_v, decimals=3))\n",
    "   # print(\"outputs_v:\",outputs_v.shape,np.around(outputs_v, decimals=3))\n",
    "   # print(\"num_examples_v:\",num_examples_v.shape,np.around(num_examples_v, decimals=3))\n",
    "   # print(\"W_out_v:\",W_out_v.shape,np.around(W_out_v, decimals=3))\n",
    "   # print(\"w_exp_v:\",w_exp_v.shape,np.around(w_exp_v, decimals=3))\n",
    "   # print(\"W_repeated_v:\",W_repeated_v.shape,np.around(W_repeated_v, decimals=3))\n",
    "   # print(\"b_out_v:\",b_out_v.shape,np.around(b_out_v, decimals=3))\n",
    "   # print(\"y_pred_v:\",y_pred_v.shape,np.around(y_pred_v, decimals=3))\n",
    "   # print(\"cost_v:\",cost_v.shape,np.around(cost_v, decimals=3))\n",
    "    #np.set_printoptions(precision=8) # put it back to initial value\n",
    "    # End debugging\n",
    "    # Test trained model on training data\n",
    "    predicted_vals_all= sess.run(y_pred,feed_dict={x:trainX}) \n",
    "    # Get last item in each predicted sequence:\n",
    "    predicted_vals = predicted_vals_all[:,seq_size-1]\n",
    "    \n",
    "    # Compute MSE\n",
    "    # step 1: denormalize data\n",
    "    predicted_vals_dnorm=predicted_vals*(max_dataset-min_dataset)+min_dataset\n",
    "    # step 2: get ground-truth\n",
    "    actual=dataset[seq_size:][:,0]\n",
    "    # step 3: compute MSE\n",
    "    mse = ((predicted_vals_dnorm - actual) ** 2).mean()\n",
    "    print(\"Training MSE = %10.5f\"%mse)\n",
    "    \n",
    "    # Plot predictions\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(seq_size,seq_size+len(predicted_vals_dnorm))), predicted_vals_dnorm, color='r', label='predicted')\n",
    "    plt.plot(list(range(len(dataset))), dataset, color='g', label='actual')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into 50% training and 50% testing\n",
    "train_size = int(len(trainX) * 0.5)\n",
    "test_size = len(trainX) - train_size\n",
    "trainX=np.asarray(trainX)\n",
    "trainY=np.asarray(trainY)\n",
    "dataX_train, dataX_test = trainX[0:train_size,:], trainX[train_size:len(trainX),:]\n",
    "dataY_train= trainY[0:train_size,:] \n",
    "print(\"Training: %i , Testing: %i\"%(len(dataX_train), len(dataX_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Session\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Run for 1000 iterations (1000 is arbitrary, need a validation set to tune!)\n",
    "    print('Training...')\n",
    "    for i in range(200): # is 1000 to much? Try 200\n",
    "        _, train_err = sess.run([train_op,cost],feed_dict={x:dataX_train,y:dataY_train})\n",
    "        if i==0:\n",
    "            print('  step, train err= %6d: %8.5f' % (0,train_err)) \n",
    "        elif  (i+1) % 100 == 0: \n",
    "            print('  step, train err= %6d: %8.5f' % (i+1,train_err)) \n",
    "\n",
    "    # Test trained model \n",
    "    predicted_vals_all_train= sess.run(y_pred,feed_dict={x:dataX_train}) \n",
    "    predicted_vals_all_test= sess.run(y_pred,feed_dict={x:dataX_test}) \n",
    "    # Get last item in each predicted sequence:\n",
    "    predicted_vals_train = predicted_vals_all_train[:,seq_size-1]\n",
    "    predicted_vals_test = predicted_vals_all_test[:,seq_size-1]\n",
    "    \n",
    "    # Compute MSE\n",
    "    # step 1: denormalize data\n",
    "    predicted_vals_dnorm_train=predicted_vals_train*(max_dataset-min_dataset)+min_dataset\n",
    "    predicted_vals_dnorm_test=predicted_vals_test*(max_dataset-min_dataset)+min_dataset\n",
    "    # step 2: get ground-truth\n",
    "    actual_train=dataset[seq_size:train_size+seq_size]\n",
    "    actual_test=dataset[seq_size+train_size:len(dataset_norm)]\n",
    "    # step 3: compute MSE\n",
    "    mse_train = ((predicted_vals_dnorm_train - actual_train) ** 2).mean()\n",
    "    mse_test = ((predicted_vals_dnorm_test - actual_test) ** 2).mean()\n",
    "    print(\"Training MSE = %10.5f\"%mse_train)\n",
    "    print(\"Testing MSE = %10.5f\"%mse_test)\n",
    "    \n",
    "    # Plot predictions\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(seq_size,seq_size+len(predicted_vals_dnorm_train))), predicted_vals_dnorm_train, color='r', label='predicted (training)')\n",
    "    plt.plot(list(range(seq_size+len(predicted_vals_dnorm_train),seq_size+len(predicted_vals_dnorm_train)+len(predicted_vals_dnorm_test))), predicted_vals_dnorm_test, color='b', label='predicted (testing)')\n",
    "    plt.plot(list(range(len(dataset))), dataset, color='g', label='actual')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare performance of RNNs, LSTMs, GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "# Encapsulate the entire prediction problem as a function\n",
    "def build_and_predict(trainX,trainY,testX,cell,cellType,input_dim=1,hidden_dim=100,seq_size = 12,max_itr=200):\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # input place holders\n",
    "        # input Shape: [# training examples, sequence length, # features]\n",
    "        x = tf.placeholder(tf.float32,[None,seq_size,input_dim])\n",
    "        # label Shape: [# training examples, sequence length]\n",
    "        y = tf.placeholder(tf.float32,[None,seq_size])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # RNN output Shape: [# training examples, sequence length, # hidden] \n",
    "        outputs, _ = tf.nn.dynamic_rnn(cell,x,dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        # weights for output dense layer (i.e., after RNN)\n",
    "        # W shape: [# hidden, 1]\n",
    "        W_out = tf.Variable(tf.random_normal([hidden_dim,1]),name=\"w_out\") \n",
    "        # b shape: [1]\n",
    "        b_out = tf.Variable(tf.random_normal([1]),name=\"b_out\")\n",
    "    \n",
    "        # output dense layer:\n",
    "        num_examples = tf.shape(x)[0] \n",
    "        # convert W from [# hidden, 1] to [# training examples, # hidden, 1]\n",
    "        # step 1: add a new dimension at index 0 using tf.expand_dims\n",
    "        w_exp= tf.expand_dims(W_out,0)\n",
    "        # step 2: duplicate W for 'num_examples' times using tf.tile\n",
    "        W_repeated = tf.tile(w_exp,[num_examples,1,1])\n",
    "        \n",
    "        # Dense Layer calculation: \n",
    "        # [# training examples, sequence length, # hidden] *\n",
    "        # [# training examples, # hidden, 1] = [# training examples, sequence length]\n",
    "        \n",
    "        y_pred = tf.matmul(outputs,W_repeated)+b_out\n",
    "        # Actually, y_pred: [# training examples, sequence length, 1]\n",
    "        # Remove last dimension using tf.squeeze\n",
    "        y_pred = tf.squeeze(y_pred)\n",
    "        \n",
    "        # Cost & Training Step\n",
    "        cost = tf.reduce_mean(tf.square(y_pred-y))\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "        \n",
    "        # Run Session\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # Run for 1000 iterations (1000 is arbitrary, need a validation set to tune!)\n",
    "        start=timeit.default_timer()\n",
    "        print('Training %s ...'%cellType)\n",
    "        for i in range(max_itr): # If we train more, would we overfit? Try 10000\n",
    "            _, train_err = sess.run([train_op,cost],feed_dict={x:trainX,y:trainY})\n",
    "            if i==0:\n",
    "                print('  step, train err= %6d: %8.5f' % (0,train_err)) \n",
    "            elif  (i+1) % 100 == 0: \n",
    "                print('  step, train err= %6d: %8.5f' % (i+1,train_err)) \n",
    "        end=timeit.default_timer()        \n",
    "        print(\"Training time : %10.5f\"%(end-start))\n",
    "         # Test trained model on training data\n",
    "        predicted_vals_all= sess.run(y_pred,feed_dict={x:testX}) \n",
    "        # Get last item in each predicted sequence:\n",
    "        predicted_vals = predicted_vals_all[:,seq_size-1]\n",
    "      \n",
    "    \n",
    "    return predicted_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=1 # dim > 1 for multivariate time series\n",
    "hidden_dim=100 # number of hiddent units h\n",
    "max_itr=2000 # number of training iterations\n",
    "\n",
    "# Different RNN Cell Types\n",
    "RNNcell = rnn.BasicRNNCell(hidden_dim)\n",
    "LSTMcell = rnn.BasicLSTMCell(hidden_dim)\n",
    "GRUcell = rnn.GRUCell(hidden_dim)\n",
    "\n",
    "# Build models and predict on testing data\n",
    "predicted_vals_rnn=build_and_predict(dataX_train,dataY_train,dataX_test,RNNcell,\"RNN\",input_dim,hidden_dim,seq_size,max_itr)\n",
    "predicted_vals_lstm=build_and_predict(dataX_train,dataY_train,dataX_test,LSTMcell,\"LSTM\",input_dim,hidden_dim,seq_size,max_itr)\n",
    "predicted_vals_gru=build_and_predict(dataX_train,dataY_train,dataX_test,GRUcell,\"GPU\",input_dim,hidden_dim,seq_size,max_itr)\n",
    "\n",
    "# Compute MSE\n",
    "# step 1: denormalize data\n",
    "predicted_vals_dnorm_rnn=predicted_vals_rnn*(max_dataset-min_dataset)+min_dataset\n",
    "predicted_vals_dnorm_lstm=predicted_vals_lstm*(max_dataset-min_dataset)+min_dataset\n",
    "predicted_vals_dnorm_gru=predicted_vals_gru*(max_dataset-min_dataset)+min_dataset\n",
    "# step 2: get ground-truth\n",
    "actual_test=dataset[seq_size+train_size:len(dataset_norm)]\n",
    "# step 3: compute MSE\n",
    "mse_rnn= ((predicted_vals_dnorm_rnn - actual_test) ** 2).mean()\n",
    "mse_lstm = ((predicted_vals_dnorm_lstm - actual_test) ** 2).mean()\n",
    "mse_gru = ((predicted_vals_dnorm_gru - actual_test) ** 2).mean()\n",
    " \n",
    "print(\"RNN MSE = %10.5f\"%mse_rnn)\n",
    "print(\"LSTM MSE = %10.5f\"%mse_lstm)\n",
    "print(\"GRU MSE = %10.5f\"%mse_gru)\n",
    "\n",
    "# Plot predictions\n",
    "pred_len=len(predicted_vals_dnorm_rnn)\n",
    "train_len=len(dataX_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(list(range(seq_size+train_len,seq_size+train_len+train_len)), predicted_vals_dnorm_rnn, color='r', label='RNN')\n",
    "plt.plot(list(range(seq_size+train_len,seq_size+train_len+train_len)), predicted_vals_dnorm_lstm, color='b', label='LSTM')\n",
    "plt.plot(list(range(seq_size+train_len,seq_size+train_len+train_len)), predicted_vals_dnorm_gru, color='y', label='GRU')\n",
    "plt.plot(list(range(len(dataset))), dataset, color='g', label='Actual')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Regularization: Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import rnn_cell \n",
    "# Encapsulate the entire prediction problem as a function\n",
    "def build_and_predict_d(trainX,trainY,testX,cell,cellType,input_dim=1,hidden_dim=100,seq_size = 12,max_itr=200,keep_prob=0.5):\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # input place holders\n",
    "        # input Shape: [# training examples, sequence length, # features]\n",
    "        x = tf.placeholder(tf.float32,[None,seq_size,input_dim])\n",
    "        # label Shape: [# training examples, sequence length]\n",
    "        y = tf.placeholder(tf.float32,[None,seq_size])\n",
    "        dropout = tf.placeholder(tf.float32)\n",
    "        \n",
    "        cell = rnn_cell.DropoutWrapper(cell)\n",
    "        # RNN output Shape: [# training examples, sequence length, # hidden] \n",
    "        outputs, _ = tf.nn.dynamic_rnn(cell,x,dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        # weights for output dense layer (i.e., after RNN)\n",
    "        # W shape: [# hidden, 1]\n",
    "        W_out = tf.Variable(tf.random_normal([hidden_dim,1]),name=\"w_out\") \n",
    "        # b shape: [1]\n",
    "        b_out = tf.Variable(tf.random_normal([1]),name=\"b_out\")\n",
    "    \n",
    "        # output dense layer:\n",
    "        num_examples = tf.shape(x)[0] \n",
    "        # convert W from [# hidden, 1] to [# training examples, # hidden, 1]\n",
    "        # step 1: add a new dimension at index 0 using tf.expand_dims\n",
    "        w_exp= tf.expand_dims(W_out,0)\n",
    "        # step 2: duplicate W for 'num_examples' times using tf.tile\n",
    "        W_repeated = tf.tile(w_exp,[num_examples,1,1])\n",
    "        \n",
    "        # Dense Layer calculation: \n",
    "        # [# training examples, sequence length, # hidden] *\n",
    "        # [# training examples, # hidden, 1] = [# training examples, sequence length]\n",
    "        \n",
    "        y_pred = tf.matmul(outputs,W_repeated)+b_out\n",
    "        # Actually, y_pred: [# training examples, sequence length, 1]\n",
    "        # Remove last dimension using tf.squeeze\n",
    "        y_pred = tf.squeeze(y_pred)\n",
    "        \n",
    "        # Cost & Training Step\n",
    "        cost = tf.reduce_mean(tf.square(y_pred-y))\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "        \n",
    "        # Run Session\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # Run for 1000 iterations (1000 is arbitrary, need a validation set to tune!)\n",
    "        start=timeit.default_timer()\n",
    "        print('Training %s ...'%cellType)\n",
    "        for i in range(max_itr): # If we train more, would we overfit? Try 10000\n",
    "            _, train_err = sess.run([train_op,cost],feed_dict={x:trainX,y:trainY,dropout:keep_prob})\n",
    "            if i==0:\n",
    "                print('  step, train err= %6d: %8.5f' % (0,train_err)) \n",
    "            elif  (i+1) % 100 == 0: \n",
    "                print('  step, train err= %6d: %8.5f' % (i+1,train_err)) \n",
    "        end=timeit.default_timer()        \n",
    "        print(\"Training time : %10.5f\"%(end-start))\n",
    "         # Test trained model on training data\n",
    "        predicted_vals_all= sess.run(y_pred,feed_dict={x:testX,dropout:1}) \n",
    "        # Get last item in each predicted sequence:\n",
    "        predicted_vals = predicted_vals_all[:,seq_size-1]\n",
    "      \n",
    "    \n",
    "    return predicted_vals\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test models after adding dropout\n",
    "input_dim=1 # dim > 1 for multivariate time series\n",
    "hidden_dim=100 # number of hiddent units h\n",
    "max_itr=2000 # number of training iterations\n",
    "keep_prob=0.5\n",
    "# Different RNN Cell Types\n",
    "RNNcell = rnn.BasicRNNCell(hidden_dim)\n",
    "LSTMcell = rnn.BasicLSTMCell(hidden_dim)\n",
    "GRUcell = rnn.GRUCell(hidden_dim)\n",
    "\n",
    "# Build models and predict on testing data\n",
    "predicted_vals_rnn=build_and_predict_d(dataX_train,dataY_train,dataX_test,RNNcell,\"RNN\",input_dim,hidden_dim,seq_size,max_itr,keep_prob)\n",
    "predicted_vals_lstm=build_and_predict_d(dataX_train,dataY_train,dataX_test,LSTMcell,\"LSTM\",input_dim,hidden_dim,seq_size,max_itr,keep_prob)\n",
    "predicted_vals_gru=build_and_predict_d(dataX_train,dataY_train,dataX_test,GRUcell,\"GPU\",input_dim,hidden_dim,seq_size,max_itr,keep_prob)\n",
    "\n",
    "# Compute MSE\n",
    "# step 1: denormalize data\n",
    "predicted_vals_dnorm_rnn=predicted_vals_rnn*(max_dataset-min_dataset)+min_dataset\n",
    "predicted_vals_dnorm_lstm=predicted_vals_lstm*(max_dataset-min_dataset)+min_dataset\n",
    "predicted_vals_dnorm_gru=predicted_vals_gru*(max_dataset-min_dataset)+min_dataset\n",
    "# step 2: get ground-truth\n",
    "actual_test=dataset[seq_size+train_size:len(dataset_norm)]\n",
    "# step 3: compute MSE\n",
    "mse_rnn= ((predicted_vals_dnorm_rnn - actual_test) ** 2).mean()\n",
    "mse_lstm = ((predicted_vals_dnorm_lstm - actual_test) ** 2).mean()\n",
    "mse_gru = ((predicted_vals_dnorm_gru - actual_test) ** 2).mean()\n",
    " \n",
    "print(\"RNN MSE = %10.5f\"%mse_rnn)\n",
    "print(\"LSTM MSE = %10.5f\"%mse_lstm)\n",
    "print(\"GRU MSE = %10.5f\"%mse_gru)\n",
    "\n",
    "# Plot predictions\n",
    "pred_len=len(predicted_vals_dnorm_rnn)\n",
    "train_len=len(dataX_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(list(range(seq_size+train_len,seq_size+train_len+train_len)), predicted_vals_dnorm_rnn, color='r', label='RNN')\n",
    "plt.plot(list(range(seq_size+train_len,seq_size+train_len+train_len)), predicted_vals_dnorm_lstm, color='b', label='LSTM')\n",
    "plt.plot(list(range(seq_size+train_len,seq_size+train_len+train_len)), predicted_vals_dnorm_gru, color='y', label='GRU')\n",
    "plt.plot(list(range(len(dataset))), dataset, color='g', label='Actual')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving & Loading a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_save_d(modelDir,trainX,trainY,cell,cellType,input_dim=1,hidden_dim=100,\n",
    "                          seq_size = 12,max_itr=200,keep_prob=0.5):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # input place holders\n",
    "        # input Shape: [# training examples, sequence length, # features]\n",
    "        x = tf.placeholder(tf.float32,[None,seq_size,input_dim],name=\"x_in\")\n",
    "        # label Shape: [# training examples, sequence length]\n",
    "        y = tf.placeholder(tf.float32,[None,seq_size],name=\"y_in\")\n",
    "        dropout = tf.placeholder(tf.float32,name=\"dropout_in\")\n",
    "        \n",
    "        cell = rnn_cell.DropoutWrapper(cell)\n",
    "        # RNN output Shape: [# training examples, sequence length, # hidden] \n",
    "        outputs, _ = tf.nn.dynamic_rnn(cell,x,dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        # weights for output dense layer (i.e., after RNN)\n",
    "        # W shape: [# hidden, 1]\n",
    "        W_out = tf.Variable(tf.random_normal([hidden_dim,1]),name=\"w_out\") \n",
    "        # b shape: [1]\n",
    "        b_out = tf.Variable(tf.random_normal([1]),name=\"b_out\")\n",
    "    \n",
    "        # output dense layer:\n",
    "        num_examples = tf.shape(x)[0] \n",
    "        # convert W from [# hidden, 1] to [# training examples, # hidden, 1]\n",
    "        # step 1: add a new dimension at index 0 using tf.expand_dims\n",
    "        w_exp= tf.expand_dims(W_out,0)\n",
    "        # step 2: duplicate W for 'num_examples' times using tf.tile\n",
    "        W_repeated = tf.tile(w_exp,[num_examples,1,1])\n",
    "        \n",
    "        # Dense Layer calculation: \n",
    "        # [# training examples, sequence length, # hidden] *\n",
    "        # [# training examples, # hidden, 1] = [# training examples, sequence length]\n",
    "        \n",
    "        y_pred = tf.matmul(outputs,W_repeated)+b_out\n",
    "        # Actually, y_pred: [# training examples, sequence length, 1]\n",
    "        # Remove last dimension using tf.squeeze\n",
    "        y_pred = tf.squeeze(y_pred,name=\"y_pred\")\n",
    "        \n",
    "        # Cost & Training Step\n",
    "        cost = tf.reduce_mean(tf.square(y_pred-y))\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "        saver=tf.train.Saver()\n",
    "        \n",
    "        # Run Session\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # Run for 1000 iterations (1000 is arbitrary, need a validation set to tune!)\n",
    "        start=timeit.default_timer()\n",
    "        print('Training %s ...'%cellType)\n",
    "        for i in range(max_itr): # If we train more, would we overfit? Try 10000\n",
    "            _, train_err = sess.run([train_op,cost],feed_dict={x:trainX,y:trainY,dropout:keep_prob})\n",
    "            if i==0:\n",
    "                print('  step, train err= %6d: %8.5f' % (0,train_err)) \n",
    "            elif  (i+1) % 100 == 0: \n",
    "                print('  step, train err= %6d: %8.5f' % (i+1,train_err)) \n",
    "            if i>0 and (i+1) % 100 == 0:    \n",
    "                modelPath= saver.save(sess,\"%s/model_%s\"%(modelDir,cellType),global_step=i+1)\n",
    "                print(\"model saved:%s\"%modelPath)    \n",
    "        end=timeit.default_timer()        \n",
    "        print(\"Training time : %10.5f\"%(end-start))\n",
    "       \n",
    "    return \n",
    "\n",
    "\n",
    "def load_and_predict(testX,modelDir,cellType,itr):\n",
    "    with tf.Session() as sess:\n",
    "        print (\"Load model:%s-%s\"%(modelDir,itr))\n",
    "        saver = tf.train.import_meta_graph(\"%s/model_%s-%s.meta\"%(modelDir,cellType,itr))\n",
    "        saver.restore(sess,tf.train.latest_checkpoint(\"%s\"%modelDir))\n",
    "        graph = tf.get_default_graph()\n",
    "        # print all nodes in saved graph \n",
    "        #print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "        # get tensors by name to use in prediction\n",
    "        x = graph.get_tensor_by_name(\"x_in:0\")\n",
    "        dropout= graph.get_tensor_by_name(\"dropout_in:0\")\n",
    "        y_pred = graph.get_tensor_by_name(\"y_pred:0\")\n",
    "        \n",
    "        predicted_vals_all= sess.run(y_pred, feed_dict={ x: testX, dropout:1})\n",
    "        # Get last item in each predicted sequence:\n",
    "        predicted_vals = predicted_vals_all[:,seq_size-1]\n",
    "    return predicted_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=1 # dim > 1 for multivariate time series\n",
    "hidden_dim=100 # number of hiddent units h\n",
    "max_itr=500 # number of training iterations\n",
    "keep_prob=0.5\n",
    "modelDir='modelDir'\n",
    "# Different RNN Cell Types\n",
    "RNNcell = rnn.BasicRNNCell(hidden_dim)\n",
    "\n",
    "# Build models and save model\n",
    "build_and_save_d(modelDir,dataX_train,dataY_train,RNNcell,\"RNN\",input_dim,hidden_dim,seq_size,max_itr,keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and predict\n",
    "predicted_vals_rnn=load_and_predict(dataX_test,modelDir,\"RNN\",max_itr)\n",
    "# Compute MSE\n",
    "# step 1: denormalize data\n",
    "predicted_vals_dnorm_rnn=predicted_vals_rnn*(max_dataset-min_dataset)+min_dataset\n",
    "# step 2: get ground-truth\n",
    "actual_test=dataset[seq_size+train_size:len(dataset_norm)]\n",
    "# step 3: compute MSE\n",
    "mse_rnn= ((predicted_vals_dnorm_rnn - actual_test) ** 2).mean()\n",
    " \n",
    "print(\"RNN MSE = %10.5f\"%mse_rnn)\n",
    "\n",
    "# Plot predictions\n",
    "pred_len=len(predicted_vals_dnorm_rnn)\n",
    "train_len=len(dataX_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(list(range(seq_size+train_len,seq_size+train_len+train_len)), predicted_vals_dnorm_rnn, color='r', label='RNN')\n",
    "plt.plot(list(range(len(dataset))), dataset, color='g', label='Actual')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
